Style	Syntax	Keyboard shortcut	Example	Output
Bold	** ** or __ __	Command+B (Mac) or Ctrl+B (Windows/Linux)	**This is bold text**	This is bold text
Italic	* * or _ _     	Command+I (Mac) or Ctrl+I (Windows/Linux)	_This text is italicized_	This text is italicized
Strikethrough	~~ ~~ or ~ ~	None	~~This was mistaken text~~	This was mistaken text
Bold and nested italic	** ** and _ _	None	**This text is _extremely_ important**	This text is extremely important
All bold and italic	*** ***	None	***All this text is important***	All this text is important

# A first-level heading
## A second-level heading
### A third-level heading




# TO-DO LIST:
1. QA Testing **TASK_2** [DONE_FOR_NOW]

2. QLib Integration Spec **TASK_4**

2. System Documenation **TASK_1**
3. Start Using Data **TASK_3**



# SELECTED:
- Verify how new_pools_history works, is this creating data that can be utilized with QLib and how is it generating statistical signals. I think this area might need more work in order to create truly predictive signals.
# -- optimize this method so it directly creates .BIN files **TASK_4**



- Confirm that metadata collection logs are still updating [PENDING] **ReviewEndToEndIntegrationTest** **TASK_2**

- Organize some files in the root folder - consolidate test coverage [NOT_STARTED] **TASK_1**



# BACKLOG:
- Integrate QLin bin methodology into architecture?
- Integrate QLib into Signal Analysis Module?
- UNICODE_ENCONDING_FIX.md ***TECHNICAL_DEBT*** [DONE]
- Test removal of watchlist entry: solana_test_pool_5233c82b [PENDING] *TroubleshootCLIUtility*
- CLI Test Results: Review CLI Options
- Current Metrics: Review metrics being tracked currently
- 

# Journal Thoughts / Head Trash Zone:

- Hard to a word in edgewise sometimes [MOTOR_TIMING_ISSUE]
- No one listens to me mraaah [MOTOR_TIMING_ISSUE]



## ###############



### DAILY STATUS UPDATE 2025-09-18 ####################################

## START HERE:
# Review: test_comprehensive_new_pools_system.py [SELECTED_FOR_REVIEW] **ReviewEndToEndIntegrationTest**

python test_comprehensive_new_pools_system.py




#### END DAILY STATUS UPDATE #################################

# DAILY STATUS UPDATE 2025-09-17 *SYSTEM_ARCHITECTURE_DIAGRAM.md*

## SYSTEM DIAGRAM NOTES ##
🏗️ System Architecture Overview
External APIs: GeckoTerminal API integration
Core Components: CLI, Config Manager, Database Manager
Data Collectors: All 7 collector types (with New Pools marked as working)
Signal Analysis Engine: Working signal analyzer and activity scorer
Database Tables: All PostgreSQL tables with current status
Testing Suite: Your comprehensive testing infrastructure

🔄 Data Flow Sequence
Shows the complete flow from CLI command → API call → Signal Analysis → Database Storage → Watchlist Integration

🗄️ Database Schema
Entity-relationship diagram showing how all tables connect, including the new_pools_history table with signal analysis fields

🧠 Signal Analysis Flow
Detailed flowchart of how pools are analyzed for trading signals and automatically added to watchlist

📋 CLI Command Structure
Mind map of all available commands organized by category

📊 Current Status Summary
✅ Working: New pools collection, signal analysis, watchlist management
🔧 Minor Issues: Unicode handling, some test queries
📈 Key Metrics: 499 records, 3 high signals detected, excellent performance

The diagram clearly shows that your "weird bugs" have been resolved - the new pools history system is now fully operational with proper signal analysis and database integration. The system is collecting data, analyzing signals, and automatically managing your watchlist based on trading opportunities.

#### END SYSTEM DIAGRAM ##########################





###### BEGIN WORKING SPACE ##################

### 2. TASK_2 (QA Testing) ################################################
## Work on Updated Test Scripts for system data workflow (QA)
## Review other bugs noted in previous QA iteration for any other data flows within the system to consider (logging, etc)

🚀 Starting Comprehensive Signal Analysis System Test
============================================================
🧪 Testing Signal Analyzer...
--------------------------------------------------
2025-09-17 23:35:45,933 - INFO - Signal analyzer initialized with thresholds: volume_spike=2.0, liquidity_growth=1.5
Test 1: High Volume Spike Pool
  Signal Score: 100.0
  Volume Trend: spike
  Liquidity Trend: stable
  Should add to watchlist: True

Test 2: Stable Pool (Low Signals)
  Signal Score: 26.2
  Volume Trend: stable
  Liquidity Trend: stable
  Should add to watchlist: False

Test 3: New Pool (No Historical Data)
  Signal Score: 68.6
  Volume Trend: unknown
  Liquidity Trend: unknown
  Should add to watchlist: False

✅ Signal Analyzer tests completed!

🔄 Testing Enhanced New Pools Collector...
--------------------------------------------------
2025-09-17 23:35:45,982 - INFO - Database connection initialized
2025-09-17 23:35:45,982 - INFO - Creating database tables
2025-09-17 23:35:46,047 - INFO - Database tables created successfully
2025-09-17 23:35:46,047 - INFO - Using PostgreSQL database - no additional optimizations needed
2025-09-17 23:35:46,048 - INFO - SQLAlchemy database manager initialized
2025-09-17 23:35:46,050 - INFO - Signal analyzer initialized with thresholds: volume_spike=2.0, liquidity_growth=1.5
✅ Collector initialized successfully
  Signal analysis enabled: True
  Auto-watchlist enabled: False
  Network: solana
2025-09-17 23:35:46,050 - INFO - Synchronous database engine disposed
2025-09-17 23:35:46,051 - INFO - SQLAlchemy database manager closed

🗄️  Testing Database Methods...
--------------------------------------------------
2025-09-17 23:35:46,057 - INFO - Database connection initialized
2025-09-17 23:35:46,057 - INFO - Creating database tables
2025-09-17 23:35:46,108 - INFO - Database tables created successfully
2025-09-17 23:35:46,110 - INFO - Using PostgreSQL database - no additional optimizations needed
2025-09-17 23:35:46,110 - INFO - SQLAlchemy database manager initialized
✅ get_pool_history: Retrieved 0 records
✅ is_pool_in_watchlist: False
2025-09-17 23:35:46,153 - INFO - Added pool solana_test_pool_2b0ffc21 to watchlist
✅ add_to_watchlist: Added test entry
✅ Verification: Pool now in watchlist: True
✅ Cleanup: Removed test data
2025-09-17 23:35:46,168 - INFO - Synchronous database engine disposed
2025-09-17 23:35:46,168 - INFO - SQLAlchemy database manager closed

⚡ Testing CLI Commands...
--------------------------------------------------
✅ analyze-pool-signals command help works
✅ monitor-pool-signals command help works

📊 Test Summary
==============================
Signal Analyzer      ✅ PASS
Enhanced Collector   ✅ PASS
Database Methods     ✅ PASS
CLI Commands         ✅ PASS
------------------------------
Total: 4/4 tests passed
🎉 All tests passed! Signal analysis system is ready.

(c:\Projects\geckoterminal_collector\.conda) C:\Projects\geckoterminal_collector>python test_comprehensive_new_pools_system.py
🧪 COMPREHENSIVE NEW POOLS SYSTEM TEST
============================================================
Started at: 2025-09-17 23:37:46.395139
🔌 Testing Database Connection
----------------------------------------
✓ Database connection successful
📋 Total watchlist entries: 12
📊 new_pools_history table exists: True
📊 Recent history records (24h): 2280

🔍 Testing New Pools Collection
----------------------------------------

🧪 Basic Collection (Dry Run)
   Command: run-collector new-pools --network solana --dry-run
✓ Success
   📊 Running new-pools collector...
   📊 Records collected: 40

🧪 Collection with Auto-Watchlist (Dry Run)
   Command: run-collector new-pools --network solana --auto-watchlist --min-liquidity 5000 --min-volume 1000 --dry-run
✓ Success
   📊 Running new-pools collector...
   📊 Records collected: 20

🧪 Enhanced Collection (Real)
   Command: collect-new-pools --network solana --auto-watchlist --min-liquidity 1000 --min-volume 100 --min-activity-score 60 --dry-run
✓ Success

📈 Testing New Pools History Data
----------------------------------------
1️⃣ Checking recent history records...
✓ Found 10 recent records
   📊 solana_6cic3wbXbeDpU... | 2025-09-17 23:37:53.535078-06:00 | Vol: $514 | Liq: $5,986 | Signal: 1.4521
   📊 solana_33CVEhx1uKyKf... | 2025-09-17 23:37:53.530074-06:00 | Vol: $37 | Liq: $5,438 | Signal: 5.0085
   📊 solana_6yAN8GNtmwjY1... | 2025-09-17 23:37:53.527380-06:00 | Vol: $2,659 | Liq: $5,545 | Signal: 8.9344

2️⃣ Checking data quality...
   📊 Total records (24h): 2320
   📊 Volume data: 2320/2320 (100.0%)
   📊 Liquidity data: 2320/2320 (100.0%)
   📊 Signal scores: 2320/2320 (100.0%)
   📊 Creation dates: 2320/2320 (100.0%)
   📊 Avg signal score: 21.0

3️⃣ Checking for potential issues...
   ! solana_7YxZkNeCUxG52...: 2 records
   ! solana_9nRbMp1XEgYf5...: 2 records
   ! solana_2GrQ1BbKAa1uh...: 2 records
   ! Issues detected:
      - Found 5 pools with duplicate records in last hour

🎯 Testing Signal Analysis
----------------------------------------

🧪 Database Health Check
✓ Success
   Command executed successfully (Unicode display issue)

📋 Testing Watchlist Integration
----------------------------------------

🧪 List Current Watchlist
✓ Success
   ID    Pool ID                                            Symbol     Name                           Active   Added
   -----------------------------------------------------------------------------------------------------------------------------
   1     solana_Cnd9CKtG6meUJqKu9NkSeriAgzPSbQpZV5qwq5B44Spz UNEMPLOYED/SOL UNEMPLOYED/SOL                 True     2025-09-15 19:01:16
   3     solana_9T8xix8dctFdJgtAcEEKeqJwqxHc7ecLdLGwTqUFCPLC Xoai / SOL Xoai / SOL                     True     2025-09-16 20:09:59
   8     solana_7bqJG2ZdMKbEkgSmfuqNVBvqEvWavgL8UEo33ZqdL3NP CBRL       Cracker Barrel Old Country S.. False    2025-09-17 05:23:07
   9     solana_4w2cysotX6czaUGmmWg13hDpY4QEMG2CzeKYEQyK9Ama TROLL      TROLL Token                    False    2025-09-17 05:23:09
   10    test_pool_05d5edea                                 TBT        Test Base Token                False    2025-09-17 07:19:53
   11    test_pool_2bfd6873                                 TBT        Test Base Token                False    2025-09-17 07:21:57
   12    test_pool_e9d54d1d                                 TBT        Test Base Token                False    2025-09-17 07:22:38
   13    solana_test_pool_5233c82b                          TEST       Test Token                     True     2025-09-17 07:38:25
   19    test_pool_123                                      TEST123    Test Token 123                 True     2025-09-17 14:59:08
   20    test_pool_372b3af9                                 TEST372b3af9 Test Token 372b3af9            False    2025-09-17 15:01:16
   21    test_pool_debug                                    TESTDEBUG  Test Debug Token               True     2025-09-17 15:01:45
   22    test_pool_8ae69303                                 TEST8ae69303 Test Token 8ae69303            False    2025-09-17 15:03:56

🧪 List Active Watchlist (JSON)
✓ Success
   [
   {
   "id": 1,
   "pool_id": "solana_Cnd9CKtG6meUJqKu9NkSeriAgzPSbQpZV5qwq5B44Spz",
   "token_symbol": "UNEMPLOYED/SOL",
   "token_name": "UNEMPLOYED/SOL",
   "network_address": "ExfkY8EwGfNkJWns2CApCGa6PsQYuJiM6NudRcFNpump",
   "created_at": "2025-09-15T19:01:16.792964-06:00",
   "is_active": true
   },
   {
   "id": 3,
   "pool_id": "solana_9T8xix8dctFdJgtAcEEKeqJwqxHc7ecLdLGwTqUFCPLC",
   "token_symbol": "Xoai / SOL",
   "token_name": "Xoai / SOL",

🧪 Analyze Pool Discovery
✓ Success
   Command executed successfully (Unicode display issue)

⚡ Testing Database Performance
----------------------------------------
1️⃣ Testing query performance...
   📊 Query returned 100 records in 0.01s
   ✓ Query performance good

2️⃣ Checking database indexes...
   📊 Found 25 indexes
   📋 new_pools_history: idx_new_pools_history_activity_score
   📋 new_pools_history: idx_new_pools_history_collected_at
   📋 new_pools_history: idx_new_pools_history_dex_id
   📋 new_pools_history: idx_new_pools_history_network_id
   📋 new_pools_history: idx_new_pools_history_pool_id
   📋 new_pools_history: idx_new_pools_history_pool_signal_time
   📋 new_pools_history: idx_new_pools_history_signal_score
   📋 new_pools_history: idx_new_pools_history_type
   📋 new_pools_history: idx_new_pools_history_volume_trend
   📋 new_pools_history: new_pools_history_pkey
   📋 new_pools_history: uq_new_pools_history_pool_collected
   📋 pools: idx_pools_activity_score_desc
   📋 pools: idx_pools_auto_discovered_at
   📋 pools: idx_pools_collection_priority
   📋 pools: idx_pools_dex_id
   📋 pools: idx_pools_discovery_source
   📋 pools: idx_pools_last_activity_check
   📋 pools: idx_pools_reserve_usd
   📋 pools: ix_pools_activity_score
   📋 pools: ix_pools_base_token_id
   📋 pools: ix_pools_collection_priority
   📋 pools: ix_pools_dex_id
   📋 pools: ix_pools_discovery_source
   📋 pools: ix_pools_quote_token_id
   📋 pools: pools_pkey

============================================================
📊 COMPREHENSIVE TEST REPORT
============================================================

📂 Database Connection
------------------------------
✓ Database Connection

📂 New Pools Collection
------------------------------
✓ Basic Collection (Dry Run)
✓ Collection with Auto-Watchlist (Dry Run)
✓ Enhanced Collection (Real)

📂 History Data Validation
------------------------------
✓ History Data Validation

📂 Signal Analysis
------------------------------
✓ Database Health Check

📂 Watchlist Integration
------------------------------
✓ List Current Watchlist
✓ List Active Watchlist (JSON)
✓ Analyze Pool Discovery

📂 Database Performance
------------------------------
✓ Database Performance

📊 SUMMARY
   Total Tests: 10
   Passed: 10
   Failed: 0
   Success Rate: 100.0%

💡 RECOMMENDATIONS
   * All tests passed! System is working well.

🔧 NEXT STEPS
   1. Review any failed tests above
   2. Check database performance if queries are slow
   3. Verify signal analysis is producing meaningful results
   4. Monitor watchlist integration for auto-additions
   5. Set up regular monitoring of new pools collection

🏁 Testing completed at: 2025-09-17 23:38:04.665124



### TASK_2 (QA Testing) -- WORKING AREA ######################

# watchlist --> new_pools_history: integrated test suite
This script tests all watchlist operations:

# Run the comprehensive test
python test_comprehensive_new_pools_system.py

# Test individual components
python examples/test_enhanced_watchlist_cli.py

# Debug specific new pools history issues
python debug_new_pools_history.py

# Run the comprehensive test (QA)
python test_comprehensive_new_pools_system.py

# Test Signal Analysis System
python test_signal_analysis_system.py

### END Testing Notes ##

### END TASK_2 ##################################











##### TASK_1 (System Documenation) WORKING AREA ########################## **SystemDocumentation**
#### Organize Documentation & Commands
# DATABASE_BACKUP_GUIDE.md
# CLI_TEST_SUITE_FIX_SUMMARY.md


python examples/cli_with_scheduler.py start
gecko-cli add-watchlist --pool-id solana_9T8xix8dctFdJgtAcEEKeqJwqxHc7ecLdLGwTqUFCPLC --symbol "Xoai / SOL" --name "Xoai / SOL" --network-address 6zRAjv9VUyouvDYuJYE8FBsj2rrhSneMr7gcZpFtafXp --active true

# Backup
python quick_backup.py

# Create Backup Before Migration:
python quick_backup.py
python simple_backup.py --list

# Run the Signal Analysis Migration:
python migrations/add_signal_fields_to_new_pools_history.py

# Test the New System:
python test_signal_analysis_system.py

# If Issues Occur, Restore:
python restore_database_backup.py ./backups/quick_backup_YYYYMMDD_HHMMSS
python -m gecko_terminal_collector.cli collect-new-pools --network solana --auto-watchlist
python simple_backup.py --list  # Check backups

# Document most important functions in system, as well as CLI architecture
# Review all documentation and organize it into a single unified resource (too scattered right now)

New CLI commands available:
  • gecko-cli add-watchlist --pool-id <id> --symbol <sym> [--name <name>] [--network-address <addr>] [--active true/false]
  • gecko-cli list-watchlist [--active-only] [--format table/csv/json]
  • gecko-cli update-watchlist --pool-id <id> [--symbol <sym>] [--name <name>] [--network-address <addr>] [--active true/false]
  • gecko-cli remove-watchlist --pool-id <id> [--force]

🚀 Starting Comprehensive CLI Test Suite
============================================================

📋 Testing Basic CLI Functionality...
----------------------------------------
✅ Main Help                      (0.18s)
✅ Version                        (0.18s)
✅ Command Structure              (0.17s)

🔍 Testing Individual Command Help...
----------------------------------------
✅ init help                      (0.18s)
✅ validate help                  (0.18s)
✅ db-setup help                  (0.20s)
✅ start help                     (0.18s)
✅ stop help                      (0.18s)
✅ status help                    (0.18s)
✅ run-collector help             (0.18s)
✅ backfill help                  (0.17s)
✅ export help                    (0.19s)
✅ cleanup help                   (0.18s)
✅ health-check help              (0.19s)
✅ metrics help                   (0.19s)
✅ logs help                      (0.18s)
✅ backup help                    (0.19s)
✅ restore help                   (0.18s)
✅ build-ohlcv help               (0.17s)
✅ validate-workflow help         (0.20s)
✅ migrate-pool-ids help          (0.19s)
✅ add-watchlist help             (0.19s)
✅ list-watchlist help            (0.20s)
✅ update-watchlist help          (0.19s)
✅ remove-watchlist help          (0.20s)
✅ collect-new-pools help         (0.18s)
✅ analyze-pool-discovery help    (0.18s)
✅ analyze-pool-signals help      (0.20s)
✅ monitor-pool-signals help      (0.20s)
✅ db-health help                 (0.19s)
✅ db-monitor help                (0.21s)

# Analyze pool signals from historical data
python gecko_terminal_collector/cli.py analyze-pool-signals --network solana --hours 24 --min-signal-score 70

# Monitor pools for real-time signal conditions  
python gecko_terminal_collector/cli.py monitor-pool-signals --network solana --alert-threshold 80 --interval 300

###### END TASK 1 ############################




##### TASK_3 WORKING AREA ####################
###
# extract trade, ohlcv data from database for CaseStudy_1 (in progress)

Based on my analysis of your current implementation, here's what would be involved in adding a comprehensive history tracking table to new_pools_data for QLib processing and predictive models:

Current State Assessment
You already have a solid foundation with:

NewPoolsHistory model with comprehensive fields
NewPoolsCollector that stores historical records
Signal analysis integration
PostgreSQL optimization with proper indexing

Required Enhancements for QLib Integration
1. Time Series Data Structure Optimization
enhanced_new_pools_history_model.py

2. Data Collection Enhancement
enhanced_new_pools_collector.py

3. QLib Integration Module
qlib_integration.py

4. Database Migration Script
migrate_to_enhanced_history.py

5. CLI Integration
cli_enhancements.py


Summary
Adding a comprehensive history tracking table to new_pools_data for QLib and predictive models involves these key components:

What's Required:
Enhanced Data Model (enhanced_new_pools_history_model.py)

Time series optimized structure with OHLC data
Advanced metrics for ML features (RSI, MACD, volatility)
QLib-compatible fields and indexing
Feature vector storage for pre-computed ML features
Enhanced Collector (enhanced_new_pools_collector.py)

Multi-interval data collection (1h, 4h, 1d)
Real-time feature engineering
Signal analysis integration
QLib symbol generation
QLib Integration (qlib_integration.py)

Data export in QLib-compatible format
Feature engineering pipeline
Model training configuration
Time series data preparation
Migration Strategy (migrate_to_enhanced_history.py)

Seamless upgrade from existing data
Data integrity validation
Backup and rollback capabilities
CLI Integration (cli_enhancements.py)

Enhanced collection commands
QLib export functionality
Model training pipeline
Performance monitoring

Key Benefits:
Time Series Ready: Proper timestamp indexing for QLib
ML Optimized: Pre-computed features and normalized data
Scalable: PostgreSQL partitioning and optimized indexes
Flexible: Multiple collection intervals and feature sets
Production Ready: Error handling, validation, and monitoring

Implementation Steps:
Deploy Enhanced Models: Create new table structures
Migrate Existing Data: Upgrade current history records
Update Collectors: Switch to enhanced collection logic
Configure QLib: Set up data export and model training
Monitor Performance: Track collection and model accuracy

The system transforms your basic new pools tracking into a comprehensive ML-ready data pipeline that can feed directly into QLib for quantitative analysis and predictive modeling.

#### END TASK_3 ##############################





### START TASK 4 ######################################## **TASK_4**

## Requirement 2 
Export data into QLib compatible bin file.

QLib bin files are imported into QLib-Server (the intended target for this data) using incremental imports which is modeled after the scripts/dump_bin.py script. These are stored as "bin" files which are basically a stack of buffers for each column of data.

See ./examples/qlib_scripts/dump_bin.py for the complete process. 

# Working example of this process:
python scripts/dump_bin.py dump_all --csv_path /Projects/wave_rider_qlib/csv_data/geckoterminal_output --qlib_dir /Projects/wave_rider_qlib/qlib_data/TESTDATA --date_field_name datetime --freq 60min --symbol_field_name symbol --include_fields open,high,low,close,volume

# This then allows for the data to be referenced using the following methodology:
provider_uri = "/Projects/wave_rider_qlib/qlib_data/TESTDATA"
qlib.init(provider_uri=provider_uri, region=REG_US)

## Technical Details:

# QLib bin files follow a very simple methodology which is outlined in the following code sample:
date_index = self.get_datetime_index(_df, calendar_list)
for field in self.get_dump_fields(_df.columns):
    bin_path = features_dir.joinpath(f"{field.lower()}.{self.freq}{self.DUMP_FILE_SUFFIX}")
    if field not in _df.columns:
        continue
    if bin_path.exists() and self._mode == self.UPDATE_MODE:
        # update
        with bin_path.open("ab") as fp:
            np.array(_df[field]).astype("<f").tofile(fp)
    else:
        # append; self._mode == self.ALL_MODE or not bin_path.exists()
        np.hstack([date_index, _df[field]]).astype("<f").tofile(str(bin_path.resolve()))

# QLib contains a helper class that can be used to check the "Health" status of created bin files, see ./examples/qlib_scripts/check_data_health.py. This Class could be utilized as a useful QA method, and is included below for reference.

class DataHealthChecker:
    """Checks a dataset for data completeness and correctness. The data will be converted to a pd.DataFrame and checked for the following problems:
    - any of the columns ["open", "high", "low", "close", "volume"] are missing
    - any data is missing
    - any step change in the OHLCV columns is above a threshold (default: 0.5 for price, 3 for volume)
    - any factor is missing
    """

    def __init__(
        self,
        csv_path=None,
        qlib_dir=None,
        freq="day",
        large_step_threshold_price=0.5,
        large_step_threshold_volume=3,
        missing_data_num=0,
    ):
        assert csv_path or qlib_dir, "One of csv_path or qlib_dir should be provided."
        assert not (csv_path and qlib_dir), "Only one of csv_path or qlib_dir should be provided."

        self.data = {}
        self.problems = {}
        self.freq = freq
        self.large_step_threshold_price = large_step_threshold_price
        self.large_step_threshold_volume = large_step_threshold_volume
        self.missing_data_num = missing_data_num

        if csv_path:
            assert os.path.isdir(csv_path), f"{csv_path} should be a directory."
            files = [f for f in os.listdir(csv_path) if f.endswith(".csv")]
            for filename in tqdm(files, desc="Loading data"):
                df = pd.read_csv(os.path.join(csv_path, filename))
                self.data[filename] = df

        elif qlib_dir:
            qlib.init(provider_uri=qlib_dir)
            self.load_qlib_data()

    def load_qlib_data(self):
        instruments = D.instruments(market="all")
        instrument_list = D.list_instruments(instruments=instruments, as_list=True, freq=self.freq)
        required_fields = ["$open", "$close", "$low", "$high", "$volume", "$factor"]
        for instrument in instrument_list:
            df = D.features([instrument], required_fields, freq=self.freq)
            df.rename(
                columns={
                    "$open": "open",
                    "$close": "close",
                    "$low": "low",
                    "$high": "high",
                    "$volume": "volume",
                    "$factor": "factor",
                },
                inplace=True,
            )
            self.data[instrument] = df
        print(df)

    def check_missing_data(self) -> Optional[pd.DataFrame]:
        """Check if any data is missing in the DataFrame."""
        result_dict = {
            "instruments": [],
            "open": [],
            "high": [],
            "low": [],
            "close": [],
            "volume": [],
        }
        for filename, df in self.data.items():
            missing_data_columns = df.isnull().sum()[df.isnull().sum() > self.missing_data_num].index.tolist()
            if len(missing_data_columns) > 0:
                result_dict["instruments"].append(filename)
                result_dict["open"].append(df.isnull().sum()["open"])
                result_dict["high"].append(df.isnull().sum()["high"])
                result_dict["low"].append(df.isnull().sum()["low"])
                result_dict["close"].append(df.isnull().sum()["close"])
                result_dict["volume"].append(df.isnull().sum()["volume"])

        result_df = pd.DataFrame(result_dict).set_index("instruments")
        if not result_df.empty:
            return result_df
        else:
            logger.info(f"There are no missing data.")
            return None

    def check_large_step_changes(self) -> Optional[pd.DataFrame]:
        """Check if there are any large step changes above the threshold in the OHLCV columns."""
        result_dict = {
            "instruments": [],
            "col_name": [],
            "date": [],
            "pct_change": [],
        }
        for filename, df in self.data.items():
            affected_columns = []
            for col in ["open", "high", "low", "close", "volume"]:
                if col in df.columns:
                    pct_change = df[col].pct_change(fill_method=None).abs()
                    threshold = self.large_step_threshold_volume if col == "volume" else self.large_step_threshold_price
                    if pct_change.max() > threshold:
                        large_steps = pct_change[pct_change > threshold]
                        result_dict["instruments"].append(filename)
                        result_dict["col_name"].append(col)
                        result_dict["date"].append(large_steps.index.to_list()[0][1].strftime("%Y-%m-%d"))
                        result_dict["pct_change"].append(pct_change.max())
                        affected_columns.append(col)

        result_df = pd.DataFrame(result_dict).set_index("instruments")
        if not result_df.empty:
            return result_df
        else:
            logger.info(f"There are no large step changes in the OHLCV column above the threshold.")
            return None

    def check_required_columns(self) -> Optional[pd.DataFrame]:
        """Check if any of the required columns (OLHCV) are missing in the DataFrame."""
        required_columns = ["open", "high", "low", "close", "volume"]
        result_dict = {
            "instruments": [],
            "missing_col": [],
        }
        for filename, df in self.data.items():
            if not all(column in df.columns for column in required_columns):
                missing_required_columns = [column for column in required_columns if column not in df.columns]
                result_dict["instruments"].append(filename)
                result_dict["missing_col"] += missing_required_columns

        result_df = pd.DataFrame(result_dict).set_index("instruments")
        if not result_df.empty:
            return result_df
        else:
            logger.info(f"The columns (OLHCV) are complete and not missing.")
            return None

    def check_missing_factor(self) -> Optional[pd.DataFrame]:
        """Check if the 'factor' column is missing in the DataFrame."""
        result_dict = {
            "instruments": [],
            "missing_factor_col": [],
            "missing_factor_data": [],
        }
        for filename, df in self.data.items():
            if "000300" in filename or "000903" in filename or "000905" in filename:
                continue
            if "factor" not in df.columns:
                result_dict["instruments"].append(filename)
                result_dict["missing_factor_col"].append(True)
            if df["factor"].isnull().all():
                if filename in result_dict["instruments"]:
                    result_dict["missing_factor_data"].append(True)
                else:
                    result_dict["instruments"].append(filename)
                    result_dict["missing_factor_col"].append(False)
                    result_dict["missing_factor_data"].append(True)

        result_df = pd.DataFrame(result_dict).set_index("instruments")
        if not result_df.empty:
            return result_df
        else:
            logger.info(f"The `factor` column already exists and is not empty.")
            return None

    def check_data(self):
        check_missing_data_result = self.check_missing_data()
        check_large_step_changes_result = self.check_large_step_changes()
        check_required_columns_result = self.check_required_columns()
        check_missing_factor_result = self.check_missing_factor()
        if (
            check_large_step_changes_result is not None
            or check_large_step_changes_result is not None
            or check_required_columns_result is not None
            or check_missing_factor_result is not None
        ):
            print(f"\nSummary of data health check ({len(self.data)} files checked):")
            print("-------------------------------------------------")
            if isinstance(check_missing_data_result, pd.DataFrame):
                logger.warning(f"There is missing data.")
                print(check_missing_data_result)
            if isinstance(check_large_step_changes_result, pd.DataFrame):
                logger.warning(f"The OHLCV column has large step changes.")
                print(check_large_step_changes_result)
            if isinstance(check_required_columns_result, pd.DataFrame):
                logger.warning(f"Columns (OLHCV) are missing.")
                print(check_required_columns_result)
            if isinstance(check_missing_factor_result, pd.DataFrame):
                logger.warning(f"The factor column does not exist or is empty")
                print(check_missing_factor_result)

## END TASK 4 #########